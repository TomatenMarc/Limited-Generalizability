{"cells":[{"cell_type":"markdown","metadata":{"id":"iWrT-13Aan5h"},"source":["# Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrkgiW86am3X"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab Notebooks/Feger/am-limited-generalizability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k74HyQfMxdyu"},"outputs":[],"source":["! pip install simpletransformers==0.70.1\n","! pip install transformers==4.41.1\n","! pip install emoji==2.4.0\n","! python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3P0T8Bqa3kp"},"outputs":[],"source":["import gc\n","import os\n","import time\n","import math\n","import torch\n","import spacy\n","import string\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from itertools import product\n","from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n","from sklearn.model_selection import BaseCrossValidator\n","from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, make_scorer\n","from sklearn.model_selection import GridSearchCV, KFold\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.exceptions import ConvergenceWarning\n","from sklearn.dummy import DummyClassifier\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n","from sklearn.tree import DecisionTreeClassifier\n","from simpletransformers.classification import ClassificationModel\n","from IPython.display import clear_output\n","from tqdm.notebook import tqdm\n","from transformers import logging as transformers_logging\n","from google.colab import runtime\n","\n","# Set logging level for transformers to ERROR\n","transformers_logging.set_verbosity_error()\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n","warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n","\n","# Generate stop words\n","nlp = spacy.load(\"en_core_web_lg\", disable=['ner', 'parser', 'textcat', 'senter'])\n","stop_words = nlp.Defaults.stop_words\n","\n","# Set several random seed for reproducibility and classification\n","random_seed = 123456789\n","\n","# Define a mapping of class names to labels\n","class2label = {\"Argument\": 1, \"No-Argument\": 0}\n","label2class = {1: \"Argument\", 0: \"No-Argument\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VxhEL3qavtm"},"outputs":[],"source":["df_sample = pd.read_csv('./data/sample_42.csv')\n","df_sample[\"label\"] = df_sample[\"label\"].replace(class2label)\n","assert not df_sample.isna().any().any()"]},{"cell_type":"markdown","metadata":{"id":"n84Iho0pcwK6"},"source":["# Methodology"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjMWaZ_Rq8MX"},"outputs":[],"source":["class TransformerModel(BaseEstimator):\n","    def __init__(self,\n","                 model_type=None,\n","                 model_name=None,\n","                 learning_rate=None,\n","                 num_train_epochs=None,\n","                 batch_size=None,\n","                 random_state=None):\n","        # Initialize parameters\n","        self.model_type = model_type\n","        self.model_name = model_name\n","        self.learning_rate = learning_rate\n","        self.num_train_epochs = num_train_epochs\n","        self.batch_size = batch_size\n","        self.random_state = random_state\n","        self.model = None  # Placeholder for the model\n","        self.args = None  # Placeholder for model arguments\n","\n","    def show_information(self):\n","        # Display model configuration information\n","        print(\"Model:\", self.model_name)\n","        print(\"Learning Rate:\", self.model.args.learning_rate)\n","        print(\"Number of Train Epochs:\", self.model.args.num_train_epochs)\n","        print(\"Train Batch Size:\", self.model.args.train_batch_size)\n","        print(\"Manual Seed:\", self.model.args.manual_seed)\n","\n","    def check_parameters(self):\n","        # Verify that all parameters are correctly set\n","        for key, value in self.args.items():\n","            model_value = getattr(self.model.args, key, None)\n","            # Ensure all parameters are initialized and match the expected values\n","            assert value is not None, f\"Parameter {key} is None (default) in the argument dict\"\n","            assert model_value is not None, f\"Parameter {key} is None (default) in the model\"\n","            assert model_value == value, f\"Parameter {key} is {model_value} but expected {value}\"\n","\n","    def fit(self, X, y):\n","        # Set up model arguments based on current parameters\n","        self.args = {\n","            'eval_batch_size': self.batch_size,\n","            'learning_rate': self.learning_rate,\n","            'manual_seed': self.random_state,\n","            'no_cache': True,\n","            'no_save': True,\n","            'num_train_epochs': self.num_train_epochs,\n","            'overwrite_output_dir': True,\n","            'save_eval_checkpoints': False,\n","            'save_model_every_epoch': False,\n","            'silent': False,\n","            'train_batch_size': self.batch_size,\n","            'use_multiprocessing': False,\n","            'use_multiprocessing_for_evaluation': False\n","        }\n","        # Create a DataFrame from the input data\n","        train_data = pd.DataFrame(list(zip(X, y)), columns=['text', 'labels'])\n","        # Initialize the model with the specified type and name, and the arguments\n","        self.model = ClassificationModel(self.model_type, self.model_name, args=self.args, use_cuda=torch.cuda.is_available())\n","        # Verify that all parameters are correctly set\n","        self.check_parameters()\n","        # Train the model and clean up memory\n","        self.model.train_model(train_data)\n","        gc.collect()\n","        return self\n","\n","    def predict(self, X):\n","        # Make predictions on the provided data\n","        predictions, _ = self.model.predict(X)\n","        return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qxt-if7akC-"},"outputs":[],"source":["class Fold(BaseCrossValidator):\n","    def __init__(self, df, source, target):\n","        # Initialize with a DataFrame, source dataset, and target dataset\n","        self.df = df\n","        self.source = source\n","        self.target = target\n","\n","    def split(self, X, y=None, groups=None):\n","        # Find indices for the training set from the source dataset\n","        train_idx = self.df[(self.df[\"dataset\"].isin(self.source)) & (self.df[\"split\"] == \"train\")].index.values\n","        # Find indices for the dev set from the target dataset\n","        dev_idx = self.df[(self.df[\"dataset\"].isin(self.target)) & (self.df[\"split\"] == \"dev\")].index.values\n","        # Yield the training and dev indices\n","        yield train_idx, dev_idx\n","\n","    def get_n_splits(self, X=None, y=None, groups=None):\n","        # Return the number of splits, which is 1 in this case\n","        return 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zH-YTa7xPA-C"},"outputs":[],"source":["class FeatureSelector(BaseEstimator, TransformerMixin):\n","    # Use when features are in a list of lists\n","    def __init__(self, key):\n","        self.key = key  # Key to select feature(s)\n","\n","    def fit(self, X, y=None):\n","        return self  # No fitting needed\n","\n","    def transform(self, X):\n","        # Select features based on key\n","        return [x[self.key] for x in X]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwHY73muAeJF"},"outputs":[],"source":["def select_features(df, features):\n","    # Use when features are in columns of a dataframe\n","    # Check if features is a single string, if so, return values as a flat list\n","    if isinstance(features, str):\n","        return df[features].tolist()\n","    # Otherwise, return values as a list of lists\n","    return df[features].values.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZuwLj-gM--A"},"outputs":[],"source":["def create_non_existing_folders(path):\n","    # Get the directory name from the given path\n","    directory = os.path.dirname(path)\n","    # Check if the directory does not exist\n","    if not os.path.exists(directory):\n","        # Create the directory and any necessary intermediate directories\n","        os.makedirs(directory)\n","    # Return the original path\n","    return path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdD3fPvLXuXZ"},"outputs":[],"source":["def run_experiment(df, model, model_type, model_name, cv, params, file_name, features):\n","    # Reset indices for consistent access\n","    df.reset_index(drop=True, inplace=True)\n","    # Create the output path and check if file already exists\n","    path = create_non_existing_folders(f\"./output/classification/{file_name}.npy\")\n","    print(path)\n","    if os.path.exists(path):\n","        print(f\"File already exists: {path}\")\n","        return\n","    # Define parameter grid for TransformerModelWrapper\n","    if isinstance(model, TransformerModel):\n","        params.update({\n","            'model_type': [model_type],\n","            'model_name': [model_name]\n","        })\n","    # Initialize and fit GridSearchCV\n","    start_time = int(time.perf_counter())\n","    clf = GridSearchCV(model, params, cv=cv, scoring='f1_macro', refit=False, verbose=3)\n","    clf.fit(select_features(df, features), df['label'].tolist())\n","    # Set and verify best parameters\n","    best_estimator = model.set_params(**clf.best_params_)\n","    for k, v in clf.best_params_.items():\n","        assert v == best_estimator.get_params()[k], f\"Parameter {k} was not set correctly.\"\n","    # Split data into training, dev, and test sets\n","    df_train = df[(df[\"dataset\"].isin(cv.source)) & (df[\"split\"] == \"train\")]\n","    df_dev = df[(df[\"dataset\"].isin(cv.target)) & (df[\"split\"] == \"dev\")]\n","    df_test = df[(df[\"dataset\"].isin(cv.target)) & (df[\"split\"] == \"test\")]\n","    # Select the features of the train, dev and test\n","    train_features = select_features(df_train, features)\n","    dev_features = select_features(df_dev, features)\n","    test_features = select_features(df_test, features)\n","    # Fit best estimator on training data\n","    best_estimator.fit(train_features, df_train[\"label\"].tolist())\n","    # Generate predictions and store them as variables\n","    df_dev[\"prediction\"] = best_estimator.predict(dev_features)\n","    df_test[\"prediction\"] = best_estimator.predict(test_features)\n","    # Generate classification reports\n","    target_names = [v for k, v in sorted(label2class.items(), key=lambda item: item[0])]\n","    report_dev = classification_report(df_dev[\"label\"], df_dev[\"prediction\"], output_dict=True, target_names=target_names)\n","    report_test = classification_report(df_test[\"label\"], df_test[\"prediction\"], output_dict=True, target_names=target_names)\n","    # Check for equivalence of grid search results and refit, but use tolerance as floating point numbers are compared\n","    assert math.isclose(clf.best_score_, report_dev[\"macro avg\"][\"f1-score\"], rel_tol=1e-8, abs_tol=1e-8)\n","    # Check the type of best_estimator and retrieve the random_state accordingly\n","    rnd_state = None\n","    if isinstance(best_estimator, Pipeline):\n","        last_step_name, last_step = best_estimator.steps[-1]\n","        rnd_state = last_step.random_state\n","    elif isinstance(best_estimator, TransformerModel):\n","        rnd_state = best_estimator.model.args.manual_seed\n","    else:\n","        rnd_state = best_estimator.random_state\n","    # Write the results for each datasize\n","    df_dev[\"prediction\"] = df_dev[\"prediction\"].replace(label2class)\n","    df_test[\"prediction\"] = df_test[\"prediction\"].replace(label2class)\n","    scores_params = [{\n","        \"model\": model_name,\n","        \"random_state\": rnd_state,\n","        \"source\": cv.source,\n","        \"target\": cv.target,\n","        \"size\": df.shape[0],\n","        \"best_params\": clf.best_params_,\n","        \"dev_report\": report_dev,\n","        \"test_report\": report_test,\n","        \"time_sec\": int(time.perf_counter()) - start_time,\n","        \"df_dev\": df_dev[[\"dataset_id\", \"prediction\"]],\n","        \"df_test\": df_test[[\"dataset_id\", \"prediction\"]]\n","    }]\n","    np.save(path, scores_params, allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ON054NaMXuXa"},"outputs":[],"source":["def calculate_progress(executed, total, start_time):\n","    # Calculate the progress percentage and round it to 2 decimal places\n","    progress = round(100 * (executed / total), 2)\n","    # Calculate the total time elapsed since start_time\n","    total_time = int(time.perf_counter()) - start_time\n","    # Convert total_time to hours, minutes, and seconds\n","    hours, remainder = divmod(total_time, 3600)\n","    minutes, seconds = divmod(remainder, 60)\n","    # Return the formatted progress and uptime string\n","    return f\"Finished: {executed}/{total} ({progress}%)\\nUptime {hours}h:{minutes}m:{seconds}s\\nCurrent:\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMPJZJJNSHON"},"outputs":[],"source":["def experiment(df, params, model, model_type, model_name, folder, experiment_type, features):\n","    assert experiment_type in [\"train-on-one-test-on-another\", \"leave-one-out\", \"to-what-transformers-pay-attention\"], \"Experiment type is not valid\"\n","    # Ensure the model is properly initialized if it's an instance of TransformerModel\n","    if isinstance(model, TransformerModel):\n","        _ = ClassificationModel(model_type, model_name, use_cuda=torch.cuda.is_available())\n","        clear_output(wait=True)\n","    # Generate a list of experiment configurations based on experiment_type\n","    unique_datasets = df[\"dataset\"].unique()\n","    if experiment_type == \"train-on-one-test-on-another\" or experiment_type == \"to-what-transformers-pay-attention\":\n","        experiments = list(product(unique_datasets, repeat=2))\n","    elif experiment_type == \"leave-one-out\":\n","        experiments = [(unique_datasets[unique_datasets != out].tolist(), out) for out in unique_datasets]\n","    assert experiments, \"Experiment must be defined\"\n","    total_experiments = len(experiments)  # Total number of experiments\n","    start_time = int(time.perf_counter())  # Record the start time for progress tracking\n","    # Iterate through each experiment configuration\n","    for executed, (source, target) in enumerate(experiments):\n","        # Generate file appendix\n","        base_path = f\"{experiment_type}/{folder}/\"\n","        path = f\"{base_path}{target.lower()}\" if experiment_type == \"leave-one-out\" else f\"{base_path}{source.lower()}-{target.lower()}\"\n","        # Ensure source and target are lists\n","        source = source if isinstance(source, list) else [source]\n","        target = target if isinstance(target, list) else [target]\n","        # Print the current progress\n","        print(calculate_progress(executed, total_experiments, start_time))\n","        # Filter the DataFrame to include only the relevant datasets for the current experiment\n","        df_ = df[df[\"dataset\"].isin(source + target)]\n","        assert sorted(df_[\"dataset\"].unique()) == sorted(set(source + target))\n","        # Initialize a custom cross-validator with the filtered DataFrame and current datasets\n","        cv = Fold(df=df_, source=source, target=target)\n","        # Run the experiment with the current configuration\n","        run_experiment(\n","            df=df_,\n","            model=model,\n","            model_type=model_type,\n","            model_name=model_name,\n","            cv=cv,\n","            params=params,\n","            file_name=path,\n","            features=features\n","        )\n","        # Clear the output and collect garbage to manage memory usage\n","        clear_output(wait=True)\n","        gc.collect()\n","    print(\"Finished\")"]},{"cell_type":"markdown","metadata":{"id":"IY8YDXSPiFLZ"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNdgU6LwwQO1"},"outputs":[],"source":["# Parameters for random prediction (random seed later changed)\n","random_params = {}\n","random_model = DummyClassifier(strategy=\"uniform\", random_state=random_seed)\n","\n","# Predefined POS tags\n","OPEN_CLASS_TAGS = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN', 'VERB']\n","CLOSED_CLASS_TAGS = ['ADP', 'AUX', 'CCONJ', 'DET', 'NUM', 'PART', 'PRON', 'SCONJ']\n","OTHER_TAGS = ['PUNCT', 'SYM', 'X']\n","predefined_pos_tags = OPEN_CLASS_TAGS + CLOSED_CLASS_TAGS + OTHER_TAGS\n","\n","# Combined parameters for decision tree and pipeline\n","dt_combined_params = {\n","    'kbst__k': [1, 2, 3, 4, 5, 6, 7, 'all'],\n","    'clf__max_depth': [1, 2, 3, 4, 5, None],\n","    'clf__criterion': [\"gini\", \"entropy\", \"log_loss\"],\n","}\n","\n","# Create the combined pipeline with merged parameters\n","dt_combined_model = Pipeline([\n","    ('union', FeatureUnion([\n","        ('pos', Pipeline([\n","            ('selector', FeatureSelector(key=0)),\n","            ('cvect', CountVectorizer(vocabulary=predefined_pos_tags, lowercase=False)) #Counts POS tags in the string representation of pos_tags using the predefined vocabulary.\n","        ])),\n","        ('num', FeatureSelector(key=slice(1, None)))\n","    ])),\n","    ('vtrsh', VarianceThreshold()),  # Remove constant pos features\n","    ('kbst', SelectKBest(f_classif)),  # Select k-best features\n","    ('clf', DecisionTreeClassifier(random_state=random_seed))\n","])\n","\n","# Parameters for transformer models, based on recommendations for text classification (GLUE) in the BERT/RoBERTa paper (random seed later changed)\n","transformer_params = {\n","    'learning_rate': [2e-5, 3e-5, 4e-5, 5e-5],\n","    'num_train_epochs': [3],\n","    'batch_size': [32]\n","}\n","transformer_model = TransformerModel(random_state=random_seed)"]},{"cell_type":"markdown","metadata":{"id":"P-iOP2BJrbJy"},"source":["# Classification"]},{"cell_type":"markdown","metadata":{"id":"ZgvpEn0Wa6b2"},"source":["## Train on one, test on another"]},{"cell_type":"markdown","metadata":{"id":"P9xT1Oeyr-NO"},"source":["### Dummy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrJNn3dtrzGm"},"outputs":[],"source":["experiment(df_sample, random_params, random_model, \"Random\", \"Random\", folder=\"sample_374318/Random\", experiment_type=\"train-on-one-test-on-another\", features=\"sentence\")"]},{"cell_type":"markdown","metadata":{"id":"lQxWmVMzr_x3"},"source":["### Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"L93Mioqpr2fe"},"outputs":[],"source":["experiment(df_sample, dt_combined_params, dt_combined_model, \"DTree\", \"DTree\", folder=f\"sample_374318/DTree\", experiment_type=\"train-on-one-test-on-another\", features=df_sample.iloc[:, 7:].columns)"]},{"cell_type":"markdown","metadata":{"id":"ar3866P3sDzW"},"source":["### Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLmEnQg5sQX3"},"outputs":[],"source":["experiment(df_sample, transformer_params, transformer_model, \"bertweet\", \"TomatenMarc/WRAPresentations\", folder=\"sample_374318/Wrap\", experiment_type=\"train-on-one-test-on-another\", features=\"sentence\")\n","experiment(df_sample, transformer_params, transformer_model, \"bert\", \"bert-base-uncased\", folder=\"sample_374318/Bert\", experiment_type=\"train-on-one-test-on-another\", features=\"sentence\")\n","experiment(df_sample, transformer_params, transformer_model, \"roberta\", \"roberta-base\", folder=\"sample_374318/Roberta\", experiment_type=\"train-on-one-test-on-another\", features=\"sentence\")\n","experiment(df_sample, transformer_params, transformer_model, \"distilbert\", \"distilbert-base-uncased\", folder=\"sample_374318/Distilbert\", experiment_type=\"train-on-one-test-on-another\", features=\"sentence\")"]},{"cell_type":"markdown","metadata":{"id":"giCVBhv5UECe"},"source":["## To what transformers pay attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClemCf3tTdkV"},"outputs":[],"source":["df_sample[\"content_words\"] = df_sample.sentence.apply(lambda row: \" \".join([token for token in row.split() if token not in stop_words and token not in string.punctuation]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DMHS2cUtUrWC"},"outputs":[],"source":["experiment(df_sample, transformer_params, transformer_model, \"bertweet\", \"TomatenMarc/WRAPresentations\", folder=\"sample_374318/Wrap\", experiment_type=\"to-what-transformers-pay-attention\", features=\"content_words\")\n","experiment(df_sample, transformer_params, transformer_model, \"bert\", \"bert-base-uncased\", folder=\"sample_374318/Bert\", experiment_type=\"to-what-transformers-pay-attention\", features=\"content_words\")\n","experiment(df_sample, transformer_params, transformer_model, \"roberta\", \"roberta-base\", folder=\"sample_374318/Roberta\", experiment_type=\"to-what-transformers-pay-attention\", features=\"content_words\")\n","experiment(df_sample, transformer_params, transformer_model, \"distilbert\", \"distilbert-base-uncased\", folder=\"sample_374318/Distilbert\", experiment_type=\"to-what-transformers-pay-attention\", features=\"content_words\")"]},{"cell_type":"markdown","metadata":{"id":"Elatf3Gs5O3a"},"source":["## Leave one out"]},{"cell_type":"markdown","metadata":{"id":"3VQIHD-hsKg8"},"source":["### Dummy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K46HrtgwsMq4"},"outputs":[],"source":["experiment(df_sample, random_params, random_model, \"Random\", \"Random\", folder=\"sample_374318/Random\", experiment_type=\"leave-one-out\", features=\"sentence\")"]},{"cell_type":"markdown","metadata":{"id":"-2PYq63isI2F"},"source":["### Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IotLSJ3xsQWU"},"outputs":[],"source":["experiment(df_sample, dt_combined_params, dt_combined_model, \"DTree\", \"DTree\", folder=f\"sample_374318/DTree\", experiment_type=\"leave-one-out\", features=df_sample.iloc[:, 7:].columns)"]},{"cell_type":"markdown","metadata":{"id":"1c51-Jy4sHdu"},"source":["### Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"a4U4UdOI5RTB"},"outputs":[],"source":["experiment(df_sample, transformer_params, transformer_model, \"bertweet\", \"TomatenMarc/WRAPresentations\", folder=\"sample_374318/Wrap\", experiment_type=\"leave-one-out\", features=\"sentence\")\n","experiment(df_sample, transformer_params, transformer_model, \"bert\", \"bert-base-uncased\", folder=\"sample_374318/Bert\", experiment_type=\"leave-one-out\", features=\"sentence\")\n","experiment(df_sample, transformer_params, transformer_model, \"roberta\", \"roberta-base\", folder=\"sample_374318/Roberta\", experiment_type=\"leave-one-out\", features=\"sentence\")\n","experiment(df_sample, transformer_params, transformer_model, \"distilbert\", \"distilbert-base-uncased\", folder=\"sample_374318/Distilbert\", experiment_type=\"leave-one-out\", features=\"sentence\")"]},{"cell_type":"markdown","source":["#### Manipulation"],"metadata":{"id":"RNEcWpm1K85r"}},{"cell_type":"code","source":["df_sample[\"content_words\"] = df_sample.sentence.apply(lambda row: \" \".join([token for token in row.split() if token not in stop_words and token not in string.punctuation]))"],"metadata":{"id":"UgqOa7QnK7q7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["experiment(df_sample, transformer_params, transformer_model, \"bertweet\", \"TomatenMarc/WRAPresentations\", folder=\"sample_42_manipulated/Wrap\", experiment_type=\"leave-one-out\", features=\"content_words\")\n","experiment(df_sample, transformer_params, transformer_model, \"bert\", \"bert-base-uncased\", folder=\"sample_42_manipulated/Bert\", experiment_type=\"leave-one-out\", features=\"content_words\")\n","experiment(df_sample, transformer_params, transformer_model, \"roberta\", \"roberta-base\", folder=\"sample_42_manipulated/Roberta\", experiment_type=\"leave-one-out\", features=\"content_words\")\n","experiment(df_sample, transformer_params, transformer_model, \"distilbert\", \"distilbert-base-uncased\", folder=\"sample_42_manipulated/Distilbert\", experiment_type=\"leave-one-out\", features=\"content_words\")"],"metadata":{"id":"NKL6WRnHLGUn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RKdAvksiSOQb"},"source":["# Clean Up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DD1V5KY2yuLs"},"outputs":[],"source":["runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["n84Iho0pcwK6","IY8YDXSPiFLZ","ZgvpEn0Wa6b2","P9xT1Oeyr-NO","lQxWmVMzr_x3","ar3866P3sDzW","giCVBhv5UECe","3VQIHD-hsKg8","-2PYq63isI2F","RKdAvksiSOQb"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}