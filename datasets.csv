Dataset,Paper,Repository,Conference,Year,Domain,Genre,Definition,Language,Format,Level,Binary,Documented,Usable-Out-Of-The-Box,Selected,Related,Considerations,Comment,Label Description
ACQUA,https://aclanthology.org/W19-4516/,https://zenodo.org/records/3237552,ArgMining,2019,Common Crawl,Mixed,Argumentative,English,.CSV,Sentence,Yes,Yes,Yes,Yes,---,"It may be overly specific because it concentrates on ""comparative sentences"", which could limit generalization given the task's objective to identify argumentative comparisons and exclude arguments that lack them.","The dataset is also known as COMPSENT-19, but we have chosen to name it ACQUA, as specified in the repository.","As there is no large publicly available cross-domain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or “wins”) / WORSE (the first item is worse or “looses”) or NONE (the sentence does not contain a comparison of the target items). The BETTER-sentences represent a pro argument in favor of the first compared item (or a con argument for the second item) while the roles are exchanged for the WORSE-sentences.
"
AMPERE,https://aclanthology.org/N19-1219/,http://xinyuhua.github.io/Resources/naacl19/,NAACL,2019,Peer Reviews,Academic,Argumentative,English,.TXT,Sentence/Sub-Sentence,Yes,Yes,Yes,Yes,---,"The data format differs from the traditional `.ann/.txt` pairing, preventing reproducible sentence splitting as described in the paper. Consequently, some sentences remain unlabeled and are classified as negative. While the paper reports 8,030 sentences with 339 negative classes, we extracted 6,971 sentences with only 242 exclusively negative. This discrepancy, resulting from different sentence processing and removing duplicates, makes the dataset too small for effective use.",Annotations are sometimes made at the sub-sentence level. Converting the data into complete sentences reduces the negative class size too much.,"We adapt the argumentative scheme from Park et al. [1] and consider proposition of the following five types and a non-argumentative (Non-Arg) type:

- Evaluation: a proposition that is not objectively verifiable and does not require any action to be performed, such as qualitative judgement and interpretation of the paper, e.g. ""The paper shows nice results on a number of small tasks.""
- Request: a proposition that is not objectively verifiable and suggests a course of action to be taken, such as recommendation and suggestion for new experiments, e.g. ""I would really like to see how the method performs without this hack.""
- Fact: a proposition that is verifiable with objective evidence, such as mathematical conclusion and common knowledge of the field, e.g. ""This work proposes a dynamic weight update scheme.""
- Reference: a proposition that refers to an objective evidence, such as URL link and citation, e.g. ""see MuseGAN (Dong et al), MidiNet (Yang et al), etc.""
- Quote: a quote from the paper or another source, e.g. ""The author wrote 'where r is lower bound of feature norm'.""
- Non-Arg: a non-argumentative discourse unit that does not contribute to the overall agenda of the review, such as greetings, metadata, and clarification questions, e.g. ""Aha, now I understand.""
"
ASRD,https://aclanthology.org/2020.findings-emnlp.243/,https://www.research.ibm.com/haifa/dept/vst/files/IBM_Debater_(R)_ArgsInASR_Findings-2020.v1.zip,EMNLP,2020,Debate Transcription,Spoken Debate,Argumentative,English,.CSV,Sentence,Yes,Yes,Yes,Yes,---,"The paper defines an argument as text that directly supports or opposes a specific topic. A clear stance, either pro or con, is essential for text to qualify as an argument. Therefore, even though they state to annotating arguments, this may still be considered a sub-subcategory (pro/con). Further, the data might be very small with 260 positive instances. We added a period at the end of each sentence in the data, as it originally contained little to no punctuation.",The annotations might be on a sub-level (pro/con) even tough they annotate arguments directly (binary class).,An argument is a piece of text which directly supports or contests the given topic. Note: having a clear stance towards the topic (either pro or against) is a critical prerequisite for a piece of text to be an argument.
CDCP,https://aclanthology.org/P17-1091/,http://www.mathcs.richmond.edu/~jpark/data/cdcp_acl17.zip,ACL,2017,regulationroom.org,Online Debate,Argumentative,English,.TXT,Sentence/Sub-Sentence,No,Yes,Yes,No,---,"The generation or extraction of labels requires more detailed interpretation, making this dataset unsuitable.","The propositions need additional analysis, as they use a custom hierarchy of labels, including value, facts, policy, and testimony, which seem more aligned with argumentation mining. This raises the challenge of how to generate binary labels at the sentence level, particularly given that neither code nor documentation is provided for this process. This is also strongly related to UKP.",
COMARG,https://aclanthology.org/W14-2107/,http://takelab.fer.hr/data/comarg/comarg.v1.tar.gz,WS,2014,procon.org/idebate.org,Online Debate,Argumentative,English,.XML,Comment,No,Yes,Yes,No,---,"There are labels that are numerical but not further defined, as in the paper they differ in their description. It is debatable if there is a real negative class.","The dataset includes comments and their corresponding arguments. The labels relate to if the comment attacks, supports or makes no use of the argument. The dataset was created for the argument recognition task, which involves identifying which arguments, from a predefined set, are used in users' comments and how they are presented. ",
EDIT,https://aclanthology.org/C16-1324/,https://doi.org/10.5281/zenodo.3254405,COLING,2016,aljazeera/foxnews/theguardian.com,Online Debate,Argumentative,English,.TXT,Token/Sentence,No,Yes,Yes,No,---,"The labels encode argumentation strategies, which are said to contribute to argumentative text. According to the paper, the six argumentative discourse units are absent in 167 cases, making the dataset too small in terms of the negative class. Additionally, it must be discussed whether all the labels truly constitute an argument, for example, when used alone in a sentence (as in AMPERE).","The dataset is thoroughly documented, and the data is well structured and compelling. The dataset is also known as Webis-Editorials-16.",
IAC,https://aclanthology.org/L12-1643/,https://github.com/sl-m-lab/Internet-Argument-Corpus/,LREC,2012,4forums.com,Online Debate,Argumentative,English,.CSV/SQL,Post,No,Yes,No,No,---,The process for constructing the labels is or binary labels are provided.,"The IAC dataset is available in two versions. The newer version (v2) is stored in a MySQL database that is challenging to access. Furthermore, the insufficient documentation for both versions makes it unclear how to generate the binary labels.",
MARG,https://aclanthology.org/2021.argmining-1.8/,https://github.com/rafamestre/m-arg_multimodal-argumentation-dataset/,ArgMining,2021,Presidential Debates,Spoken Debate,Argumentative,English,.CSV,Sentence,No,Yes,Yes,No,---,"Binary labels are hardly encoded, which might come with to much interpreation errors. They chose not to annotate claim and premise and instead used a relation-based approach, as mentioned in the README.","This dataset could be beneficial for future research by emphasizing the connections between arguments. However, it intentionally does not include annotations for specific argument components, such as major claims, claims, premises, and others. It might be possible that the binary rules may be derived by analyzing these relationships (attack/support/neighter).",
QMC,https://aclanthology.org/C18-1176/,https://www.research.ibm.com/haifa/dept/vst/files/IBM_Debater_(R)_claim_sentences_search.zip,COLING,2018,Wikipedia,Encyclopedia,Argumentative,English,.CSV,Sentence,Yes,Yes,Yes,Yes,---,"Here, it is important to note that the term argumentative refers to the subset of sentences that contain a claim.","The dataset does not have a specific name and is referred to as the Argument Search Engine Dataset in the Readme. However, its files use the term q_mc, which we adopt as its name. The annotated data was used as a test set in the referenced paper. It was labeled using rule-based (weak supervision) methods, but the code is not provided.","Context Dependent Claim - a general, concise statement that directly supports or contests the given Topic (we henceforth use the term claim instead of Context Dependent Claim).
"
SDAT,https://aclanthology.org/2022.nlp4pi-1.5/,https://github.com/danielhers/sustainable-diet-arguments-twitter,NLP4PI,2022,Twitter,Twitter Debate,Argumentative,English,.TXT,Tweet,Yes,No,Yes,Yes,---,"To small, with 210 negative examples.","This dataset is relatively small, containing only 597 tweets. Additionally, neither the paper nor the repository offers adequate documentation on how to handle the annotations.","Argumentative is the label that denotes if a tweet is argumentative for any topic. This means the tweet contains argumentative structures such as claims or evidence while having a clear stance on some topic. We define arguments broadly, including those that do not refer to the topic explicitly but whose stance toward it is only implied. Indeed, Wojatzki and Zesch (2016) achieved a similar result by using stance detection as a proxy. If an argument is not clear in its stance, i.e., it is neutral or unrelated, it is not be considered argumentative."
WEBIS,https://aclanthology.org/N16-1165/,https://zenodo.org/records/3251804,NAACL,2016,idabate.org,Online Debate,Argumentative,English,.TXT,Sentence,Yes,Yes,Yes,Yes,---,"This data contains pseudo-labels generated by labeling functions. Additionally, the original text is unknown, and the data has been pre-processed (lowercased). Furthermore, the numbers as specified do not match those that can be extracted from the repository.","The data was labeled using rule-based (distant supervision) methods, although the code is not available.","Based on the structure exemplified in Table 1, we stipulate on the following assumptions to automatically map components from the debate portal to annotated argumentativeness instances.

[Component]: Introduction

– [Assumption]: The introduction explains the

topic and gives important background information in a non-argumentative way.

– [Mapping]: Each sentence in the introduction is an instance of the non-argumentative class.

[Component]: Points for & Points against
– [Assumption]: Each point from these lists represents an argument for or against the stance on the topic of discussion.

– [Mapping]: Each point is an instance of the argumentative class.

[Component]: Point & Counterpoint
– [Assumption]: The main objective of a point (counterpoint) is to justify (attack) the point in the points-for or points-against list it refers to. We assume that the intention of such a point is to provide reasons for / against an argument.

– [Mapping]: Each sentence in a point / counter-point is an instance of the argumentative class.
"
AAE,https://aclanthology.org/C14-1142/,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2421,COLING,2014,essayforum.com,Academic,Claim-based,English,.ANN,Token/Sentence,Yes,Yes,Yes,No,PE,This is basically a subset of PE.,"The paper does not specify the annotation level, but the data includes sub-sentence annotations and lacks sufficient documentation. The .ANN format suggests that binary labels could be generated. Since the dataset lacks a specific name and seems to be an early version for persuasive essays (PE), we have referred to it as PEI.",
ABSTRCT,https://ecai2020.eu/papers/1470_paper,https://gitlab.com/tomaye/abstrct,ECAI,2020,PubMed,Academic,Claim-based,English,.ANN,Sentence/Sub-Sentence,Yes,Yes,Yes,Yes,---,"We made several key considerations: sentences without labels in the .ann files and stand-alone premises were classified as negative, while sentences containing a claim, including those with both a claim and a premise, were classified as positive. Although the paper referenced 500 abstracts, we extracted 700 from the repository. Additionally, applying sentence splitting may have introduced discrepancies. The paper identified 1,390 claims at the sentence or sub-sentence level, but our reconstruction of entire sentences resulted in 1,308 sentences containing claims after removing duplicates. Additionally, the splitting of sentences may not always be ideal, as medical abbreviations and numerical data often include characters typically used to end sentences (e.g, 2.9 mm Hg (95% CI: 2.3-3.6)).","Annotations can be made within parts of a sentence. An argument is composed of a claim accompanied by optional premises, while premises that stand alone do not form an argument. This is the extended version of RCT.","In the context of RCT abstracts, a claim is a concluding statement made by the author about the outcome of the study. It generally describes the relation of a new treatment (intervention arm) with respect to existing treatments (control arm) and is derived from the described results.

Major claims are more a general/concluding claim, which is supported by more specific claims. The concluding statements do not have to occur at the end of the abstract, and may also occur at the beginning of the text as an introductory claim, as in Example 1. Given the negligible occurrences of major claims in our dataset, we merge them with the claims for the classification task."
AMECHR,https://aclanthology.org/L18-1640/,https://github.com/PLN-FaMAF/ArgumentMiningECHR,LREC,2018,ECHR Court Decisions,Legal,Claim-based,English,.ANN,Sentence/Sub-Sentence,Yes,Yes,No,No,---,"This requires significant additional work to extract the majority labels, as it was not originally designed to be a training corpus.","The dataset contains individual samples from four annotators. To determine a majority vote, further considerations are required, as the data's main purpose was to demonstrate in the paper that annotation reproducibility can be achieved through agreement and improved guidelines, rather than serving as a dedicated dataset. Additionally, since the data does not have an official name, we will refer to it as AMECHR.",
AMSR,https://ojs.aaai.org/index.php/AAAI/article/view/16607,https://zenodo.org/records/4314390,AAAI,2021,OpenReview,Academic,Claim-based,English,.CSV,Sentence,Yes,Yes,Yes,Yes,---,"This dataset may be too specific, as it focuses on pro/con claims, where instances in the negative class might include examples that are neutral but still qualify as claims.","Annotations were done at the token level, but the authors aggregated them at the sentence level.","We use a simple argumentation scheme proposed in Stab, Miller, and Gurevych (2018), which distinguishes between non-arguments, supporting arguments and attacking arguments, which we denote as NON/PRO/CON accordingly."
ARGUMINSCI,https://www.aclweb.org/anthology/W18-5206/,http://data.dws.informatik.uni-mannheim.de/sci-arg/compiled_corpus.zip,ArgMining,2018,Dr. Inventor,Academic,Claim-based,English,.ANN,Sentence/Sub-Sentence,Yes,No,Yes,Yes,---,"The paper does not specify exact numbers but cites a total of 8,196 claim components. However, after merging sub-sentences into sentence-level annotations, we reduced this number to 6,554. We also classified unlabeled or stand-alone data sentences as part of the negative class. Furthermore, the .ann data is occasionally poorly formatted, so we excluded these instances during extraction. The .txt data is a parsed .xml file, and although it's unclear if this was intentional, we decided to retain it as is. The dataset is not documented at all.","The linked Git repository is designed for launching the ArguminSci system, but the data is also available as a secondary source via the institute's server. However, neither source offers documentation on how to use the data or code. They reference the Toulmin Model (claim-data) for argument components, which we interpret as a flat claim-premise structure. The annotations are provided at both the sentence and sub-sentence levels. However, the TXT files containing the original text, necessary for interpreting the annotation (ANN) files, seem to have been converted from XML to TXT format. This conversion requires further investigation to ensure proper extraction and data cleaning. Additionally, the dataset is an extension of the Dr. Inventor dataset.","Our final annotation scheme has the following types of argumentative component:
(1) Own Claim is an argumentative statement that closely relates to the authors’ own work, e.g.: ”Furthermore, we show that by simply changing the initialization and target velocity, the same optimization procedure leads to running controllers.”
(2) Background Claim is an argumentative statement relating to the background of authors’ work, e.g., about related work or common practices in the respective research field, e.g.: ”Despite the efforts, accurate modeling of human motion remains a challenging tasks.”"
ASC,https://www.linguistics.rub.de/konvens16/pub/39_konvensproc.pdf,https://github.com/RobinSchaefer/tweet-stance-classification,KONVENS,2016,Twitter,Twitter Debate,Claim-based,English,.JSON,Tweet,Yes,Yes,Yes,Yes,---,"This is about atheism and further topics are mentioned, but they can not be extracted from the data.",This might be a very small dataset.,"As a stance can always be transformed into a claim which can be considered as the minimum constituent of an argument (Habernal et al., 2014; Palau and Moens, 2009), we argue that the minimal information that has to be provided in a persuasive utterance is a stance towards some target."
CDC,https://aclanthology.org/W14-2109/,https://www.research.ibm.com/haifa/dept/vst/files/IBM_Debater_(R)_CE-ACL-2014.v0.zip,WS,2014,Wikipedia,Encyclopedia,Claim-based,English,.XLS,Sentence/Subsentence,Yes,Yes,Yes,No,CE,This is an older (smaller) version of CE.,"The dataset is outdated and is (partially) stored in CE. The sparse documentation mentions two CSV files for claim sentences, but these files are absent from the repository. Instead, there are Excel files.",
CE,https://aclanthology.org/D15-1050/,https://www.research.ibm.com/haifa/dept/vst/files/IBM_Debater_(R)_CE-EMNLP-2015.v3.zip,EMNLP,2015,Wikipedia,Encyclopedia,Claim-based,English,.TXT,Sentence/Sub-Sentence,Yes,Yes,Yes,Yes,---,"Data extraction involved developing custom matching rules. Moreover, all sentences lacking a claim were assigned to the negative class, and the dataset is considerably large. The paper reports 1,734 claims spanning 58 topics, with an average of 150 sentences per 547 articles, resulting in approximately 82k sentences. Using our sentence-level label extraction method, we identified 1,546 claims across roughly 86k sentences.","The dataset is poorly documented and does not come with accompanying code. However, claims could potentially be extracted by splitting the articles into individual sentences and matching those that contain claims. Since the dataset lacks an official name, we referenced the name of the zip file. This version is a newer and updated/extended version of CEI.","These concepts were earlier defined in (Aharoni et al., 2014) and we use the same definitions here. [...] Claim: a general, concise statement that directly supports or contests the topic."
CMV,https://aclanthology.org/W17-5102/,https://github.com/chridey/change-my-view-modes,ArgMining,2017,Reddit.com,Online Debate,Claim-based,English,.ANN,Sentence/Sub-Sentence,Yes,Yes,Yes,Yes,---,"The data in the repository do not fully match the description in the paper. In the v2.0 folder, there are 49 positive and 64 negative files, while the paper indicates only 39 annotated files for each class. Additionally, we classified sentences without labels and stand-alone premises as part of the negative class.","This dataset lacks comprehensive documentation, and its annotations are limited to the sentence or sub-sentence level.","Proposition that expresses the speaker’s stance on a certain matter. They can express predictions (‘I think that the left wing will win the election”), interpretations (“John probably went home”), evaluations (“Your choice is a bad one”) as well as agreement/disagreement with other peoples claims (“I agree”/“I think you are totally wrong”). Complex sentences where speakers at first agree and then disagree with other speakers’ opinion (concessions) constitute separate claims (“I agree with you that the environmental consequences are bad, but I still think that freedom is more important.”)."
CS,https://aclanthology.org/E17-1024/,https://www.research.ibm.com/haifa/dept/vst/files/IBM_Debater_(R)_CS_EACL-2017.v1.zip,EACL,2017,Wikipedia,Encyclopedia,Claim-based,English,.CSV/TXT,Sentence/Sub-Sentence,Yes,Yes,Yes,No,CE,This is an enhancement that adds stance (Pro/Con) to the claims of CE.,"This dataset directly references CEI and indicates that an updated version was used, which is most likely CE, as it mentions 55 topics, closely aligning with the original CE paper. Additionally, the data from CE and CS overlap.",
DT,https://aclanthology.org/2020.lrec-1.130/,https://github.com/Rav830/DiscussionTrackerCollaboration-LREC2020,LREC,2020,Debate Transcription,Spoken Debate,Claim-based,English,.XLSX,Talk,Yes,Yes,Yes,No,---,The data is organized at the paragraph level rather than the sentence level.,"While this data is interesting and includes proper documentation, it is unclear how to extract the labeled data at the sentence level, as there is no accompanying code or detailed instructions. Additionally, the Excel format requires further clarification, as annotations are provided at varying levels, sometimes at the sub-sentence level, other times at the sentence level, and occasionally at the paragraph level.",
FINARG,https://aclanthology.org/2022.finnlp-1.22/,https://github.com/Alaa-Ah/The-FinArg-Dataset-Argument-Mining-in-Financial-Earnings-Calls,FinNLP,2022,Earning Conference Calls,Spoken Debate,Claim-based,English,.ANN,Sentence/Sub-Sentence,Yes,Yes,Yes,Yes,---,"In this dataset, the .ann files include all labeled elements, including negative classes. The .json files provide text, but not all content is considered for annotation. Since no code is available for obtaining sentence-level annotations, we reconstructed complete sentences ourselves from the .ann files. Thereby, we labeled all sentences as argument containing at least one part labeled as claim. Additionally, while the paper mentions using 804 documents, we were able to extract 836.","Annotations are occasionally made at the sub-sentence level. While the repository lacks detailed documentation, the paper provides a clear description of the data structure. However, the annotation guidelines and labels, including tags such as claim-fact and claim-opinion, are not documented. The dataset follows a one-claim-per-argument approach with optional premises, while standalone premises and those labeled as non-arg are categorized as non-arguments.",Not further defined.
IAM,https://aclanthology.org/2022.acl-long.162/,https://github.com/LiyingCheng95/IAM/,ACL,2022,Online Forums,Mixed,Claim-based,English,.TXT,Sentence,Yes,Yes,Yes,Yes,---,The dataset is very big.,"This dataset is substantial, comprising 66,524 sentences after removing duplicates and is nearly three times the size of the UKP dataset.","A context-dependent claim (CDC), claim in short, is a general and concise statement that directly supports or contests the given topic (Levy et al., 2014). The annotators are asked to extract the claims by following this definition."
MT,https://aclanthology.org/D15-1110/,https://github.com/peldszus/arg-microtexts,EMNLP,2015,Handwritten Microtext,Microtext,Claim-based,German/English,.XML,Sentence,Yes,Yes,https://aclanthology.org/D17-1218,Yes,---,"This dataset comes from a second-level source, and it is important to note that the data fully matches the numbers in the original paper. However, it contains only 112 claim sentences, which is too small.","Aside from the repository's basic documentation, there is no information about the data or how to extract binary labels. However, we discovered a survey paper that cites the data (MT) and refers to it as a secondary source.",Not further defined.
OC,https://www.worldscientific.com/doi/epdf/10.1142/S1793351X11001328,http://www.cs.columbia.edu/~orb/code_data/persuasion_data.zip,IJSC,2011,LiveJournal,Online Debate,Claim-based,English,.XMI,Sentence,Yes,No,https://aclanthology.org/D17-1218,Yes,---,"This dataset is from a second-level source, where it has to be noted that this might be a subset of the original work. The original paper mentions 8,946 sentences, but the data appears to include sub-sentence annotations. We extracted 8,528 sentences from the second-level source. Additionally, some examples indicate the authors used a similar merging approach to ours for the other datasets, which reduces the size when constructing full sentences.","Although the paper does not adequately reference the data, we assume that the provided link contains the necessary data mentioned in the paper. While the data is not usable out-of-the-box, we identified a survey paper where the data is utilized and cited (OC) as a secondary source.",Not further defined.
PE,https://aclanthology.org/J17-3005/,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2422,CL,2017,essayforum.com,Academic,Claim-based,English,.ANN,Token/Sentence,Yes,Yes,https://aclanthology.org/D17-1218,Yes,---,"This dataset is from a second-level source, with the contributing authors also involved in the primary source, allowing us to reuse their data from the secondary source. The original paper mentions 7,116 sentences, but the data appears to include sub-sentence annotations. We extracted 7,051 sentences from the second-level source. Additionally, some examples indicate the authors used a similar merging approach to ours for the other datasets, which reduces the size when constructing full sentences.","While the paper claims token-level annotations, the data actually contains sub-sentence annotations and lacks sufficient documentation. This dataset, the latest version of PEI, is presented in a survey paper as PE and is sometimes referred to as AAEC.","The claim is a controversial statement and the central component of an argument, and premises are reasons for justifying (or refuting) the claim.

For determining the agreement of the stance attribute, we follow the same methodology as for the sentence-level agreement described above, but we consider each sentence containing a claim as “for” or “against” according to its stance attribute, and all sentences without a claim as “none” (N = 1,441; n = 3; k = 3)."
QT,https://aclanthology.org/2022.lrec-1.352/,http://corpora.aifdb.org/qt30,LREC,2022,BBC Question Time,Spoken Debate,Claim-based,English,.JSON/RDF,Sentence,No,No,No,No,AIFDB,"This is included in AIFDB, where the same issues as those in AIFDB arise.","Due to the lack of documentation and the data being part of the AIFDB, the same issues persist, making it highly unlikely that this dataset is usable without further investigation. This dataset is also referred to as QT30.",
RCT,https://ebooks.iospress.nl/publication/50186,https://www.dropbox.com/sh/9ms8v2blq8zstep/AAC0lGJ3PxF1L7W8ESv8R0hVa?dl=0,COMMA,2018,PubMed,Academic,Claim-based,English,.ANN,Sentence/Sub-Sentence,Yes,Yes,Yes,No,ABSTRCT,This is included in ABSTRCT.,"Although the dataset lacks thorough documentation, the data is mostly self-explanatory. Furthermore, it serves as the precursor to ABSTRCT.",
SCIARK,https://aclanthology.org/2021.argmining-1.10/,https://github.com/afergadis/SciARK,ArgMining,2021,United Nations' Sustainable Development Goals,Academic,Claim-based,English,.JSON,Sentence,Yes,Yes,Yes,Yes,---,"We extracted the data as outlined in the paper, and after removing duplicate annotations, we were left with 11,695 sentences out of the original 12,374. Additionally, we classified all claim sentences as part of the positive class, while those not labeled as claims were assigned to the negative class.","While this repository lacks comprehensive documentation, it seems intuitive and easy to understand. Additionally, it utilizes ABSTRCT data, which, when compared, is closely aligned in terms of numbers with our approach. However, our data more closely matches the original numbers presented in the paper, where they report 1,169 claim sentences, while we have 1,378 out of 1,390 claim sentences in ABSTRCT.",Claim: an argumentative statement that reports the study findings and derives from the author’s original work.
UGWD,https://aclanthology.org/J17-1004/,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2423,CL,2017,Mixed,Online Debate,Claim-based,English,.XMI,Token/Sentence,Yes,Yes,Yes,No,WD,This is basically WD.,"The data lacks adequate documentation, making the extraction of sentence-level annotations unclear, despite references to the creation of non-argument sentences. While it appears to be WD, the dataset does not have a specific name, so we have labeled it UGWD based on its repository title, Argument Annotated User-Generated Web Discourse.","Claim is “the conclusion we seek to establish by our argu- ments” (Freeley and Steinberg, 2008, p. 153) or “the assertion put forward publicly for general ac- ceptance” (Toulmin et al., 1984, p. 29)."
USELEC,https://aclanthology.org/P19-1463/,https://github.com/ElecDeb60To16/Dataset,ACL,2019,debates.org,Spoken Debate,Claim-based,English,.ANN/.CSV,Sentence/Sub-Sentence,Yes,No,Yes,Yes,---,"The data includes a .csv file with sentence-level annotations, eliminating the need for data extraction via .ann. The paper states that approximately 29.5k sentences are labeled, and after removing duplicates, we were left with 29k.","Although there is no documentation or code provided, the dataset is quite interesting and self-explanatory. It is also known as USElecDeb60To16v.01, but we use a shorter name for convenience.","Claims. Being them the ultimate goal of an argument, in the context of political debates, claims can be a policy advocated by a party or a candidate to be undertaken which needs to be justified in order to be accepted by the audience. [...] Taking a stance towards a controversial subject, or an opinion towards a specific issue is also considered as a claim (e.g., “I’ve opposed the death penalty during all of my life”). The presence of discourse indicators (e.g., “in my opinion”, “I believe”) is generally a useful hint in finding claims that state opinions and judgments.
"
VACC,https://aclanthology.org/2020.lrec-1.611/,https://github.com/cltl/vaccination-corpus,LREC,2020,Mixed,Online Debate,Claim-based,English,.CONLL,Token/Sentence,Yes,Yes,Yes,Yes,---,---,"The dataset includes a script for converting token-level annotations to sentence-level annotations. While the paper mentions 4,606 claims in the annotation process, we found this number includes duplicates and does not reflect the cleaned total. By reviewing all sentences, we determined that the actual number of unique claims is 4,508, as we were able to extract the other data as outlined in the paper.","The definition of claim chosen was the following: The claim is the central component of an argument. Claims are sections of text that express the stance of the author. Sometimes, claims are introduced by an explicit source in the text (different from the author). Since they are opinionated statements with respect to the topic, claims are often introduced by stance expressions, such as “In my opinion”, “I think that”. An important requirement is that the claim has to be a refutable statement. The following do not qualify as claims: rhetorical questions (“Wouldn’t it be better to develop immunity naturally?”), backing (“I am a nurse.”), common ground (“Measles can spread through airborne transmission.”), statistics (“80% of vaccinated children experience serious side effects.”), anecdotes (“I experienced hearing loss after being given the MMR vaccine.”), opinions (“I am against vaccinations.”)."
VG,https://aclanthology.org/L08-1553/,https://www.arg.tech/index.php/research/araucariadb/,LREC,2008,Mixed,Mixed,Claim-based,English,.AML/XML,Sentence,Yes,Yes,https://aclanthology.org/D17-1218/,Yes,AIFDB,"This dataset is from a second-level source, where it has to be noted that this might be a subset of the original work. The original paper mentions 2,842 sentences, but the data appears to include sub-sentence annotations. We extracted 2,576 sentences from the second-level source. Additionally, some examples indicate the authors used a similar merging approach to ours for the other datasets, which reduces the size when constructing full sentences.","Following a security breach, ARAUCARIADB has been taken offline. However, its extensions remain accessible through AIFDB, though AIFDB is not immediately usable. Furthermore, we found that the dataset is frequently referred to as VG in a related survey paper, which includes a repository that acts as a secondary source.",Not further defined.
WD,https://aclanthology.org/D15-1255,https://github.com/habernal/emnlp2015,EMNLP,2015,Mixed,Online Debate,Claim-based,English,.XMI,Token/Sentence,Yes,Yes,https://aclanthology.org/D17-1218,Yes,---,"This dataset is from a second-level source, with the contributing authors also involved in the primary source, allowing us to reuse their data from the secondary source. The original paper mentions 3,899 sentences, but the data appears to include sub-sentence annotations. We extracted 3,872 sentences from the second-level source. Additionally, some examples indicate the authors used a similar merging approach to ours for the other datasets, which reduces the size when constructing full sentences. The dataset may be too small, with only 211 claim sentences, which matches the number in the original paper.","The paper discusses the alignment of token-level BIO annotations with sentence boundaries. However, extracting these labels is unclear because of undocumented Java code, which necessitates a translation into Python and raises concerns about label aggregation. We did, however, find a survey paper by the original authors that refers to the dataset as WD and offers sentence-level labeled data as a secondary source. Further in the paper it is mentioned that they took the data from their previous work.","There are five different components in this model, namely, the claim (the statement about to be established in the argument which conveys author’s stance towards the topic), [...]. We made two observations in the data: the claim is often implicit (must be inferred by the reader), and some sentences have no argumentative function (thus are not labeled by any argument component)"
WTP,https://www.worldscientific.com/doi/epdf/10.1142/S1793351X11001328,http://www.cs.columbia.edu/~orb/code_data/persuasion_data.zip,IJSC,2011,Wikipedia Talks,Online Debate,Claim-based,English,.XMI,Sentence,Yes,No,https://aclanthology.org/D17-1218,Yes,---,"This dataset is from a second-level source, where it has to be noted that this might be a subset of the original work. The original paper mentions 9,140 sentences, but the data appears to include sub-sentence annotations. We extracted 8,410 sentences from the second-level source. Additionally, some examples indicate the authors used a similar merging approach to ours for the other datasets, which reduces the size when constructing full sentences.","Although the paper does not adequately reference the data, we assume that the provided link contains the necessary data mentioned in the paper. While the data is not usable out-of-the-box, we identified a survey paper where the data is utilized and cited (WTP) as a secondary source.",Not further defined.
ECHR,https://aclanthology.org/2020.argmining-1.8/,http://www.di.uevora.pt/~pq/echr/echr_corpus.zip,ArgMining,2020,ECHR Court Decisions,Legal,Conclusion-based,English,.JSON,Sentence/Sub-Sentence,Yes,No,Yes,Yes,---,"They claim to have 743 conclusions, but we found that there are only 623 unique conclusions. We were able to extract 414 unique sentences that were labeled as containing one or more conclusions.","The dataset is suitable; however, the figures presented in the paper may not correspond to the actual extracted data, as the extraction code is not provided.",Not further defined.
AFS,https://aclanthology.org/W16-3636/,http://nldslab.soe.ucsc.edu/afs16/Sigdial_16_release_data.zip,SIGDIAL,2016,procon.org/idebate.org,Online Debate,Conclusion-based,English,.CSV,Sentence,Yes,No,Yes,Yes,IAC,"We were able to extract all the data as outlined in the paper, and after removing duplicates, we were left with 6,186 out of the original 6,188 instances.","The data is not explicitly documented, but the paper indicates that the arguments were selected for their clarity in presenting a majority vote, making the data somewhat self-explanatory, as it includes yes and no votes. However, the distinction between the postId and sentenceId is not clearly defined.",The AQ score is intended to reflect how easily the speaker’s argument can be understood from the sentence without any context.
ARGSME,https://link.springer.com/chapter/10.1007/978-3-030-30179-8_4,https://zenodo.org/records/4139439,KI,2019,debatewise/idebate/debatepedia/debate.org,Online Debate,Conclusion-based,English,.JSON,Token/Sentence,No,Yes,Yes,No,---,There is no negative class.,"The dataset consists solely of arguments, making it impossible to create or extract a negative class.",
BASN,https://aclanthology.org/2021.argmining-1.11/,https://github.com/mynlp/basn,ArgMining,2021,Mixed,Mixed,Conclusion-based,English,.CSV,Sentence,No,No,Yes,No,---,"It does not include both labels, and the data is generated and biased towards the argument class, containing pairs.",The repository lacks thorough documentation and seems to concentrate on a method for generating and analyzing arguments.,
BIOARG,https://aclanthology.org/W18-5213/,https://github.com/greennl/BIO-Arg/,ArgMining,2018,CRAFT,Academic,Conclusion-based,English,.TXT/XML,Sentence,No,No,Yes,No,---,They annotated 15 arguments with the purpose of proposing a new method for annotating scientific arguments.,"This dataset is challenging to interpret due to the absence of documentation and the fact that the data itself is not self-explanatory. Additionally, it contains only one article from the CRAFT corpus.",
DEMOSTHENES,https://aclanthology.org/2022.argmining-1.14/,https://github.com/adele-project/demosthenes,ArgMining,2022,CJEU,Legal,Conclusion-based,English,.XML,Sentence,Yes,Yes,No,No,---,"The code for generating sentence-level annotations mixes Linux and Windows path separators (`/` and `\\`), which need to be corrected. Once this is fixed, encoding errors arise that require further attention. Additionally, the sentence-level annotations undergo preprocessing with stop-words removed, but more work is needed to complete the full sentence annotations. Currently, only sentences labeled as premises or conclusions are included, leaving out those without a label, which requires further investigation to extract from the `.XML` files. Also, the paper mentiones that there are only 160 conclusions, making this dataset very small.","The repository lacks detailed documentation but provides a brief guide on how to run the accompanying scripts for data creation. However, following these instructions leads to several errors in the code that requires further debugging.",
RSA,https://aclanthology.org/W14-2103/,https://github.com/hospice/rethorical_self_annotating_corpora,WS,2014,PubMed,Academic,Conclusion-based,English,.TXT,Sentence,No,Yes,Yes,No,---,"There is no negative label, and the data is primarily focused on demonstrating how their method operates.","The dataset lacks both documentation and code. It contains files that categorize sentences as conclusion, method, or result, which suggests the possibility of creating binary labels. However, this approach requires further exploration. Additionally, we chose the name ourselves.",
AIFDB,http://arg.tech/people/chris/publications/2012/comma2012aifdb.pdf,http://corpora.aifdb.org,COMMA,2012,Mixed,Mixed,AIF,Mixed,.JSON/RDF,Sentence,No,Yes,No,No,---,The process for constructing the labels is or binary labels are provided.,"The process of extracting binary labels is unclear, as the dataset seems to only contain arguments. Furthermore, there is no formal API, and the project primarily functions as a repository for analysis and the creation of custom datasets.",
LAMECHR,https://dl.acm.org/doi/10.1007/s10506-023-09361-y,https://github.com/trusthlt/mining-legal-arguments,AIL,2023,ECHR Court Decisions,Legal,Custom Framework,English,.XML/CSV,Token/Sentence,No,Yes,Yes,No,---,"This is related to the ECHR. The data is annotated at the paragraph or token level, rather than the sentence level. The proposed scheme, when properly applied in law, requires more than one sentence for accurate implementation. Additionally, the 33 BIO tags and 16 argument types necessitate detailed analysis, with certain elements, such as subsumptions, requiring further clarification. While the scheme was originally designed for normative texts, it has been adapted to fit court decisions. A lawyer who reviewed the data confirmed this, pointing out that these argument types contradict traditional definitions of an argument. Furthermore, constructing negative classes presents a challenge when they contain stand-alone labels that are not exclusively marked as O.","The data is annotated at the token level using a customized BIO tagging scheme, specifically developed for legal arguments in ECHR court decisions. However, its effectiveness for extracting binary labels is still uncertain. Furthermore, a second repository containing the complete data related to the work, which exceeds 70GB in size, is available at: https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/3601.",
ABAM,https://aclanthology.org/2020.argmining-1.5/,https://github.com/trtm/ABAM/tree/main,ArgMining,2020,Common Crawl,Mixed,Evidence or Reasoning,English,.TSV,Sentence/Sub-Sentence,No,Yes,Yes,No,AURC,"They use AURC. The extraction of labels is unclear, and there is no documentation available. The paper mentions in footnote 3 that, theoretically, there should be a negative class. However, upon inspecting the label sequences, the data does not include such a class, as there are always 'p' or 'c' in the stance (which we interpret as positive and contra), and in most cases, '1' in the aspect (which we interpret as indicating the presence of an aspect).","This task extension integrates Aspect Term Extraction (ATE) and Nested Segmentation (NS) into the AURC dataset. While the dataset may be usable, the process for extracting binary labels remains unclear, as the labels are neither defined nor documented.",
ASPECT,https://aclanthology.org/P19-1054/,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/1998,ACL,2019,Google Search,Mixed,Evidence or Reasoning,English,.TSV,Sentence,No,Yes,Yes,No,UKP,This is only about similarity between argument aspects and does not contain any negative class.,This is basically UKP but reduced to argument aspects and their similarity.,
AURC,https://ojs.aaai.org/index.php/AAAI/article/view/6438,https://github.com/trtm/AURC,AAAI,2020,Common Crawl,Mixed,Evidence or Reasoning,English,.TSV,Token/Sentence,Yes,Yes,No,No,---,"The issue cannot be reproduced due to an error, which can be interpreted as the warcio library failing to parse the downloaded data because it is in XML format instead of the expected WARC or ARC format.","This dataset is designed for sentence-level classification and includes detailed documentation and code to automatically retrieve the data from a non-public server. However, the download.sh script currently encounters an unresolved error that requires further attention, which is difficult to address due to the repository being read-only. Additionally, the definitions and data used in the dataset appear to be in relation to those of UKP. Furthermore, while the paper refers to the dataset as AURC-8, we follow the terminology used in the repository.",
BWS,https://aclanthology.org/2021.naacl-main.28/,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2496.2,NAACL,2021,Google Search,Mixed,Evidence or Reasoning,English,.CSV,Sentence,No,Yes,Yes,No,UKP,This is only about similarity between arguments and does not contain any negative class.,This is basically UKP but reduced to arguments and their similarity.,
UKP,https://aclanthology.org/D18-1402/,https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2345,EMNLP,2018,Google Search,Mixed,Evidence or Reasoning,English,.TSV,Sentence,Yes,Yes,Yes,Yes,---,"The data includes arguments supporting or opposing a given topic, which can be seen as a stance, with neutral arguments potentially contributing to the negative class, as demonstrated in the examples of the paper. We were unable to use all of the data as outlined in the dataset after it was obtained from the authors, as they appear to track deleted text.",This is one of the most well-known datasets for argument mining from heterogeneous sources. Access to the dataset must be requested directly from the authors.,"We define an argument as a span of text expressing evidence or reasoning that can be used to either support or oppose a given topic. An argument need not be “direct” or self-contained it may presuppose some common or domain knowledge, or the application of commonsense reasoning but it must be unambiguous in its orientation to the topic."
AEC,https://aclanthology.org/W15-4631/,http://nldslab.soe.ucsc.edu/arg-extraction/sigdial2015/,SIGDIAL,2015,createdebate.com,Online Debate,Implicit-Markup,English,.CSV,Sentence,Yes,Yes,Yes,Yes,IAC,This data is labeled by heuristics.,The corpus is composed of existing entries from the Internet Argument Corpus (IAC) and new posts collected from createdebate.com.,Our IMPLICIT MARKUP hypothesis posits that arguments that are good candidates for extraction will be marked by cues from the surface realization of the arguments.
TACO,https://aclanthology.org/2024.lrec-main.1349/,https://github.com/TomatenMarc/TACO,LREC-COLING,2024,Twitter,Twitter Debate,Inference-Information,English,.CSV,Tweet,Yes,Yes,Yes,Yes,---,"Sometimes, this may be more than one sentence, as tweets do not always form complete sentences or use proper sentence punctuation.",The dataset is available upon request from the authors or can be rehydrated using the Twitter API.,"To define this critical component of an argument, we refer to the Cambridge Dictionary, which defines inference as a guess that you make or an opinion that you form based on the information that you have."
